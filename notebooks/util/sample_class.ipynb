{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25766880",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0366b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from typing import List\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6cc091",
   "metadata": {},
   "source": [
    "## **Load Dataset and rename columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b176ec11",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DATA_PATH = \"../../data/nlp/Sentiment-analysis-for-mental-health.csv\"\n",
    "AUGMENTED_ANXIETY_CLASS_DATA_PATH = \"../../data/nlp/anxiety_class_augmented.csv\"\n",
    "AUGMENTED_TAGLISH_ANXIETY_CLASS_DATA_PATH = \"../../data/nlp/class_sampled_taglish_augmented.csv\"\n",
    "DATA_OUPUT_PATH = \"../../data/nlp/class_sampled.csv\"\n",
    "CLEAN_DATASET = \"../../data/nlp/clean-sentiment-analysis-for-mental-health.csv\"\n",
    "CLEAN_SAMPLED_ANXIETY_DATASET = \"../../data/nlp/clean-anxiety_class_augmented.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26188a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: tweet_emotions\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 53043\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load two datasets\n",
    "dataset = load_dataset(\"csv\", data_files=BASE_DATA_PATH)\n",
    "dataset_2 = load_dataset(\"csv\", data_files = AUGMENTED_ANXIETY_CLASS_DATA_PATH)\n",
    "\n",
    "# Define functions to rename datasets\n",
    "def rename_data_columns(data: Dataset, column_names_to_change: List[str], new_column_names: List[str]) -> Dataset:\n",
    "    if len(column_names_to_change) != len(new_column_names):\n",
    "        raise ValueError(\"Both lists must have the same length.\")\n",
    "    \n",
    "    dataset = data\n",
    "    for old_name, new_name in zip(column_names_to_change, new_column_names):\n",
    "        dataset = dataset.rename_column(old_name, new_name)\n",
    "\n",
    "    print(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Remove unwanted column\n",
    "dataset = dataset.remove_columns([\"Unnamed: 0\"])\n",
    "\n",
    "# Rename\n",
    "print(\"\\nDataset: tweet_emotions\")\n",
    "\n",
    "dataset = rename_data_columns(\n",
    "    data=dataset,\n",
    "    column_names_to_change=['status', 'statement'],\n",
    "    new_column_names=['labels', 'text']\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678b240",
   "metadata": {},
   "source": [
    "## **Clean text and remove special characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a085917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i made huge embarrassing baka mistake at work ...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i had naman an syempre worries attack during a...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do i have a personality i dont feel like i hav...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>possible sobra health anxiety parang maybe ser...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>venting a little baka info new to baka the com...</td>\n",
       "      <td>Anxiety</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text   labels\n",
       "0  i made huge embarrassing baka mistake at work ...  Anxiety\n",
       "1  i had naman an syempre worries attack during a...  Anxiety\n",
       "2  do i have a personality i dont feel like i hav...  Anxiety\n",
       "3  possible sobra health anxiety parang maybe ser...  Anxiety\n",
       "4  venting a little baka info new to baka the com...  Anxiety"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(example):\n",
    "    text = example.get(\"text\", \"\")\n",
    "\n",
    "    if text is None or not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {\"text\": \"\"}   # keep it empty; will be dropped later\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Remove newlines and other non-word characters except spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Remove words containing numbers\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(clean_text)\n",
    "dataset_2 = dataset_2.map(clean_text)\n",
    "\n",
    "dataset[\"train\"].to_pandas().head()\n",
    "dataset_2[\"train\"].to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd9455",
   "metadata": {},
   "source": [
    "## **Check if there are any null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a51932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking split: train in sentiment analysis for mental health\n",
      "\n",
      "Nulls in each column:\n",
      "text      0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Rows where text or labels are null/blank:\n",
      "Empty DataFrame\n",
      "Columns: [text, labels]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Check for blank and empty string values:\n",
      "Blank texts: 369\n",
      "Blank labels: 0\n",
      "\n",
      "\n",
      "Checking split: train in anxiety class sampled\n",
      "\n",
      "Nulls in each column:\n",
      "text      0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Rows where text or labels are null/blank:\n",
      "Empty DataFrame\n",
      "Columns: [text, labels]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Check for blank and empty string values:\n",
      "Blank texts: 0\n",
      "Blank labels: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_for_null_blank(dataset, dataset_name=\"Dataset\"):\n",
    "    import pandas as pd\n",
    "\n",
    "    # If it's a DatasetDict, loop over splits\n",
    "    if hasattr(dataset, \"keys\"):  # DatasetDict\n",
    "        for split in dataset.keys():\n",
    "            print(f\"Checking split: {split} in {dataset_name}\\n\")\n",
    "            df = dataset[split].to_pandas()\n",
    "            \n",
    "            # Nulls\n",
    "            print(\"Nulls in each column:\")\n",
    "            print(df[[\"text\", \"labels\"]].isnull().sum())\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Rows with nulls\n",
    "            print(\"Rows where text or labels are null/blank:\")\n",
    "            print(df[df[\"text\"].isnull() | df[\"labels\"].isnull()])\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Blank strings\n",
    "            print(\"Check for blank and empty string values:\")\n",
    "            print(\"Blank texts:\", (df[\"text\"].str.strip() == \"\").sum())\n",
    "            print(\"Blank labels:\", (df[\"labels\"].astype(str).str.strip() == \"\").sum())\n",
    "            print(\"\\n\")\n",
    "    else:  # Single Dataset\n",
    "        df = dataset.to_pandas()\n",
    "        print(f\"Checking {dataset_name}\\n\")\n",
    "        print(\"Nulls in each column:\")\n",
    "        print(df[[\"text\", \"labels\"]].isnull().sum())\n",
    "        print(\"\\n\")\n",
    "        print(\"Rows where text or labels are null/blank:\")\n",
    "        print(df[df[\"text\"].isnull() | df[\"labels\"].isnull()])\n",
    "        print(\"\\n\")\n",
    "        print(\"Check for blank and empty string values:\")\n",
    "        print(\"Blank texts:\", (df[\"text\"].str.strip() == \"\").sum())\n",
    "        print(\"Blank labels:\", (df[\"labels\"].astype(str).str.strip() == \"\").sum())\n",
    "        print(\"\\n\")\n",
    "\n",
    "check_for_null_blank(dataset=dataset, dataset_name=\"sentiment analysis for mental health\")\n",
    "check_for_null_blank(dataset=dataset_2, dataset_name=\"anxiety class sampled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0476e7",
   "metadata": {},
   "source": [
    "## **Drop null values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec69bbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned train: 53043 → 52674 rows\n",
      "✅ Cleaned train: 2000 → 2000 rows\n",
      "Checking split: train in Dataset\n",
      "\n",
      "Nulls in each column:\n",
      "text      0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Rows where text or labels are null/blank:\n",
      "Empty DataFrame\n",
      "Columns: [text, labels, __index_level_0__]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Check for blank and empty string values:\n",
      "Blank texts: 0\n",
      "Blank labels: 0\n",
      "\n",
      "\n",
      "Checking split: train in Dataset\n",
      "\n",
      "Nulls in each column:\n",
      "text      0\n",
      "labels    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Rows where text or labels are null/blank:\n",
      "Empty DataFrame\n",
      "Columns: [text, labels]\n",
      "Index: []\n",
      "\n",
      "\n",
      "Check for blank and empty string values:\n",
      "Blank texts: 0\n",
      "Blank labels: 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def drop_nulls_from_dataset(dataset, columns=[\"text\", \"labels\"]):\n",
    "    \"\"\"\n",
    "    Drops null and blank values from a Dataset or DatasetDict.\n",
    "    Works for both cases.\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, DatasetDict):  # Case 1: DatasetDict with train/test\n",
    "        cleaned_splits = {}\n",
    "        for split_name, ds_split in dataset.items():\n",
    "            df = ds_split.to_pandas()\n",
    "            df = df.dropna(subset=columns)\n",
    "            if \"text\" in columns:\n",
    "                df = df[df[\"text\"].str.strip() != \"\"]\n",
    "            cleaned_splits[split_name] = Dataset.from_pandas(df)\n",
    "            print(f\"✅ Cleaned {split_name}: {len(ds_split)} → {len(df)} rows\")\n",
    "        return DatasetDict(cleaned_splits)\n",
    "\n",
    "    elif isinstance(dataset, Dataset):  # Case 2: Single Dataset\n",
    "        df = dataset.to_pandas()\n",
    "        df = df.dropna(subset=columns)\n",
    "        if \"text\" in columns:\n",
    "            df = df[df[\"text\"].str.strip() != \"\"]\n",
    "        cleaned_dataset = Dataset.from_pandas(df)\n",
    "        print(f\"✅ Cleaned dataset: {len(dataset)} → {len(df)} rows\")\n",
    "        return cleaned_dataset\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Expected Dataset or DatasetDict\")\n",
    "\n",
    "dataset = drop_nulls_from_dataset(dataset=dataset)\n",
    "dataset_2 = drop_nulls_from_dataset(dataset=dataset_2)\n",
    "\n",
    "# Check counts again\n",
    "check_for_null_blank(dataset);\n",
    "check_for_null_blank(dataset_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa7e41",
   "metadata": {},
   "source": [
    "## **Check if there are duplicate rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d57199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking duplicates in split: train (Sentiment analysis for mental health)\n",
      "\n",
      "Duplicates by `text` only: 3205 rows\n",
      "                                                 text   labels  \\\n",
      "18  no regrets or grudgesangry at things that have...  Anxiety   \n",
      "39  but my heart is still restless even though my ...  Anxiety   \n",
      "53                              restless and restless  Anxiety   \n",
      "56                                   why am i nervous  Anxiety   \n",
      "78                              restless and agitated  Anxiety   \n",
      "\n",
      "    __index_level_0__  \n",
      "18                 18  \n",
      "39                 39  \n",
      "53                 53  \n",
      "56                 56  \n",
      "78                 78  \n",
      "\n",
      "\n",
      "Duplicates by `text` + `labels`: 3167 rows\n",
      "                                                 text   labels  \\\n",
      "18  no regrets or grudgesangry at things that have...  Anxiety   \n",
      "39  but my heart is still restless even though my ...  Anxiety   \n",
      "53                              restless and restless  Anxiety   \n",
      "56                                   why am i nervous  Anxiety   \n",
      "78                              restless and agitated  Anxiety   \n",
      "\n",
      "    __index_level_0__  \n",
      "18                 18  \n",
      "39                 39  \n",
      "53                 53  \n",
      "56                 56  \n",
      "78                 78  \n",
      "\n",
      "\n",
      "🔍 Checking duplicates in split: train (Anxiety class sampled)\n",
      "\n",
      "Duplicates by `text` only: 41 rows\n",
      "                                                  text   labels\n",
      "27   ive been feeling restless for the past few day...  Anxiety\n",
      "128                                            anxious  Anxiety\n",
      "189                im most worried when my mom is sick  Anxiety\n",
      "238                  ya allah cant sleep im so worried  Anxiety\n",
      "386                 please worry until you want to cry  Anxiety\n",
      "\n",
      "\n",
      "Duplicates by `text` + `labels`: 41 rows\n",
      "                                                  text   labels\n",
      "27   ive been feeling restless for the past few day...  Anxiety\n",
      "128                                            anxious  Anxiety\n",
      "189                im most worried when my mom is sick  Anxiety\n",
      "238                  ya allah cant sleep im so worried  Anxiety\n",
      "386                 please worry until you want to cry  Anxiety\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_for_duplicates(dataset, dataset_name=\"Dataset\", text_column=\"text\", label_column=\"labels\"):\n",
    "    \"\"\"\n",
    "    Checks for duplicate rows in a Hugging Face Dataset or DatasetDict.\n",
    "    Detects duplicates based on text only, or text+label if specified.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(dataset, DatasetDict):  # Case 1: DatasetDict\n",
    "        for split_name, ds_split in dataset.items():\n",
    "            df = ds_split.to_pandas()\n",
    "            print(f\"🔍 Checking duplicates in split: {split_name} ({dataset_name})\\n\")\n",
    "            \n",
    "            # Duplicates by text only\n",
    "            dup_text = df[df.duplicated(subset=[text_column], keep=False)]\n",
    "            print(f\"Duplicates by `{text_column}` only: {len(dup_text)} rows\")\n",
    "            if not dup_text.empty:\n",
    "                print(dup_text.head())\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Duplicates by text + label (exact row duplicates)\n",
    "            dup_both = df[df.duplicated(subset=[text_column, label_column], keep=False)]\n",
    "            print(f\"Duplicates by `{text_column}` + `{label_column}`: {len(dup_both)} rows\")\n",
    "            if not dup_both.empty:\n",
    "                print(dup_both.head())\n",
    "            print(\"\\n\")\n",
    "\n",
    "    elif isinstance(dataset, Dataset):  # Case 2: Single Dataset\n",
    "        df = dataset.to_pandas()\n",
    "        print(f\"🔍 Checking duplicates in {dataset_name}\\n\")\n",
    "        \n",
    "        # Duplicates by text only\n",
    "        dup_text = df[df.duplicated(subset=[text_column], keep=False)]\n",
    "        print(f\"Duplicates by `{text_column}` only: {len(dup_text)} rows\")\n",
    "        if not dup_text.empty:\n",
    "            print(dup_text.head())\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        # Duplicates by text + label (exact row duplicates)\n",
    "        dup_both = df[df.duplicated(subset=[text_column, label_column], keep=False)]\n",
    "        print(f\"Duplicates by `{text_column}` + `{label_column}`: {len(dup_both)} rows\")\n",
    "        if not dup_both.empty:\n",
    "            print(dup_both.head())\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        raise TypeError(\"Expected Dataset or DatasetDict\")\n",
    "    \n",
    "check_for_duplicates(dataset, dataset_name=\"Sentiment analysis for mental health\")\n",
    "check_for_duplicates(dataset_2, dataset_name=\"Anxiety class sampled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de622afa",
   "metadata": {},
   "source": [
    "## **Drop Duplicated Rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b464fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned train: 52674 → 49469 rows (removed 3205)\n",
      "✅ Cleaned train: 2000 → 1959 rows (removed 41)\n",
      "🔍 Checking duplicates in split: train (Dataset)\n",
      "\n",
      "Duplicates by `text` only: 0 rows\n",
      "\n",
      "\n",
      "Duplicates by `text` + `labels`: 0 rows\n",
      "\n",
      "\n",
      "🔍 Checking duplicates in split: train (Dataset)\n",
      "\n",
      "Duplicates by `text` only: 0 rows\n",
      "\n",
      "\n",
      "Duplicates by `text` + `labels`: 0 rows\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def drop_duplicates_from_dataset(dataset, text_column=\"text\", by_text_only=True):\n",
    "    \"\"\"\n",
    "    Drops duplicate rows from a Hugging Face Dataset or DatasetDict.\n",
    "\n",
    "    Args:\n",
    "        dataset: Dataset or DatasetDict\n",
    "        text_column: column to check for duplicates\n",
    "        by_text_only: \n",
    "            - True  = drop rows where the same text appears more than once (remove ALL copies)\n",
    "            - False = drop rows where the same text+label pair appears more than once (remove ALL copies)\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned Dataset or DatasetDict\n",
    "    \"\"\"\n",
    "    from datasets import Dataset, DatasetDict\n",
    "\n",
    "    def _clean(df):\n",
    "        # Decide grouping\n",
    "        subset_cols = [text_column] if by_text_only else [text_column, \"labels\"]\n",
    "\n",
    "        # Find duplicates → mark all occurrences\n",
    "        dup_mask = df.duplicated(subset=subset_cols, keep=False)\n",
    "\n",
    "        # Keep only unique rows\n",
    "        df_cleaned = df[~dup_mask].reset_index(drop=True)\n",
    "\n",
    "        removed = len(df) - len(df_cleaned)\n",
    "        return df_cleaned, removed\n",
    "\n",
    "    if isinstance(dataset, DatasetDict):  # Case 1: DatasetDict\n",
    "        cleaned_splits = {}\n",
    "        for split_name, ds_split in dataset.items():\n",
    "            df = ds_split.to_pandas()\n",
    "            df_cleaned, removed = _clean(df)\n",
    "            print(f\"✅ Cleaned {split_name}: {len(df)} → {len(df_cleaned)} rows (removed {removed})\")\n",
    "            cleaned_splits[split_name] = Dataset.from_pandas(df_cleaned, preserve_index=False)\n",
    "        return DatasetDict(cleaned_splits)\n",
    "\n",
    "    elif isinstance(dataset, Dataset):  # Case 2: Single Dataset\n",
    "        df = dataset.to_pandas()\n",
    "        df_cleaned, removed = _clean(df)\n",
    "        print(f\"✅ Cleaned dataset: {len(df)} → {len(df_cleaned)} rows (removed {removed})\")\n",
    "        return Dataset.from_pandas(df_cleaned, preserve_index=False)\n",
    "\n",
    "    else:\n",
    "        raise TypeError(\"Expected Dataset or DatasetDict\")\n",
    "\n",
    "# Remove ALL rows where text is duplicated (even with different labels)\n",
    "dataset = drop_duplicates_from_dataset(dataset, by_text_only=True)\n",
    "dataset_2 = drop_duplicates_from_dataset(dataset_2, by_text_only=True)\n",
    "\n",
    "# Check again\n",
    "check_for_duplicates(dataset)\n",
    "check_for_duplicates(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87da453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# Drop the unwanted index column if it exists\n",
    "if \"__index_level_0__\" in df.columns:\n",
    "    df = df.drop(columns=[\"__index_level_0__\"])\n",
    "\n",
    "# Save clean CSV\n",
    "df.to_csv(CLEAN_DATASET, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "df_2 = dataset_2[\"train\"].to_pandas()\n",
    "\n",
    "# Drop the unwanted index column if it exists\n",
    "if \"__index_level_0__\" in df.columns:\n",
    "    df_2 = df_2.drop(columns=[\"__index_level_0__\"])\n",
    "\n",
    "# Save clean sampled anxiety class csv\n",
    "df_2.to_csv(CLEAN_SAMPLED_ANXIETY_DATASET, index=False, encoding=\"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36a0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_base_cleaned = pd.read_csv(CLEAN_DATASET, encoding=\"utf-8-sig\")\n",
    "# df_anxiety_augmented = pd.read_csv(CLEAN_SAMPLED_ANXIETY_DATASET, encoding=\"utf-8-sig\")\n",
    "# df_anxiety_augmented_taglish = pd.read_csv(AUGMENTED_TAGLISH_ANXIETY_CLASS_DATA_PATH, encoding=\"utf-8-sig\")\n",
    "# df_stress_aug = pd.read_csv('../../data/nlp/stress_class_sampled_augmented.csv', encoding=\"utf-8-sig\")\n",
    "# df_stress_aug_v2 = pd.read_csv('../../data/nlp/stress_class_sampled_augmented_v2.csv', encoding=\"utf-8-sig\")\n",
    "\n",
    "df_past_combined = pd.read_csv('../../data/nlp/combined_cleaned_dataset_v2.csv', encoding=\"utf-8-sig\")\n",
    "augmented_suicidal = pd.read_csv('../../data/nlp/augmented_suicidal_depression_confused_cases_paraphrased.csv', encoding=\"utf-8-sig\")\n",
    "\n",
    "# Drop unwanted columns\n",
    "augmented_suicidal = augmented_suicidal.drop(columns=['text', 'pred'], errors='ignore')\n",
    "\n",
    "# Rename columns\n",
    "augmented_suicidal = augmented_suicidal.rename(columns={\n",
    "    'true': 'labels',\n",
    "    'paraphrase': 'text'\n",
    "})\n",
    "\n",
    "# Combine all\n",
    "df_combined = pd.concat([df_past_combined, augmented_suicidal], ignore_index=True)\n",
    "\n",
    "# Save back\n",
    "df_combined.to_csv(\"combined_cleaned_dataset.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26624920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       labels                                               text\n",
      "0    Suicidal  I really want to die and I might carry it out ...\n",
      "1  Depression  I'm overwhelmed with sadness and can't seem to...\n",
      "2  Depression  I've been feeling numb and utterly hopeless ab...\n",
      "3  Depression  Lately i've been feeling very down and empty, ...\n",
      "4  Depression  Lately i've been feeling very down and empty, ...\n"
     ]
    }
   ],
   "source": [
    "print(augmented_suicidal.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a295b0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_class_to_csv(dataset, label_name, num_rows, output_path, label_column=\"labels\", seed=42):\n",
    "    \"\"\"\n",
    "    Samples rows from a given class (by label name, e.g. \"Anxiety\")\n",
    "    and saves them to CSV.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset or DatasetDict): Hugging Face dataset.\n",
    "        label_name (str): The target class to filter, e.g. \"Anxiety\".\n",
    "        num_rows (int): Number of rows to sample.\n",
    "        output_path (str): Path to save CSV.\n",
    "        label_column (str): Column name for labels (should contain string labels).\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    # Convert dataset to single DataFrame\n",
    "    if isinstance(dataset, DatasetDict):\n",
    "        if \"train\" not in dataset:\n",
    "            raise ValueError(\"DatasetDict must contain a 'train' split or specify which split to use.\")\n",
    "        df = dataset[\"train\"].to_pandas()\n",
    "    elif isinstance(dataset, Dataset):\n",
    "        df = dataset.to_pandas()\n",
    "    else:\n",
    "        raise TypeError(\"Expected Dataset or DatasetDict\")\n",
    "\n",
    "    # Filter rows for the specific class\n",
    "    class_rows = df[df[label_column] == label_name]\n",
    "\n",
    "    if class_rows.empty:\n",
    "        raise ValueError(f\"No rows found for label '{label_name}'\")\n",
    "\n",
    "    # Sample randomly\n",
    "    sampled = class_rows.sample(\n",
    "        n=min(num_rows, len(class_rows)),\n",
    "        random_state=seed\n",
    "    )\n",
    "\n",
    "    # Save to CSV\n",
    "    sampled.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"✅ Saved {len(sampled)} rows for class '{label_name}' to {output_path}\")\n",
    "\n",
    "    return sampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2a78651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 2000 rows for class 'Stress' to ../../data/nlp/class_sampled.csv\n"
     ]
    }
   ],
   "source": [
    "# Suppose dataset is your HF dataset\n",
    "sampled_df = sample_class_to_csv(\n",
    "    dataset=dataset,\n",
    "    label_name=\"Stress\",                 # class to sample (e.g., minority class)\n",
    "    num_rows=2000,               # number of rows to take\n",
    "    output_path=DATA_OUPUT_PATH\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heron-wellnest-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
