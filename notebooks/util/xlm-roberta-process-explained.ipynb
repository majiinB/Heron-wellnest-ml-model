{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6798ef16",
   "metadata": {},
   "source": [
    "# XLM-RoBERTa Inference Pipeline — Process Explained\n",
    "\n",
    "This notebook explains, step by step, how XLM-RoBERTa (xlm-roberta-base) processes text for multi-class classification — from raw text, through tokenization and tensors, into the Transformer, and finally to probabilities and predicted labels.\n",
    "\n",
    "It focuses on concepts and shapes rather than heavy computations or training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ad1950",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "- How text is normalized and tokenized (SentencePiece BPE)\n",
    "- How inputs become tensors: input_ids and attention_mask\n",
    "- How batches are formed and fed to the model\n",
    "- What happens inside the model (embeddings → Transformer blocks)\n",
    "- How the classification head produces logits and probabilities\n",
    "- How this maps to your Phase 2 training/evaluation notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1066c",
   "metadata": {},
   "source": [
    "## End-to-end flow (high level)\n",
    "1) Raw text → normalized (basic unicode normalization).\n",
    "2) Tokenized with SentencePiece BPE → subword pieces (e.g., `▁hello`, `world`).\n",
    "3) Special tokens are added: `<s>` (start), `</s>` (end), and padding if needed.\n",
    "4) Convert tokens → integer IDs (input_ids).\n",
    "5) Create attention_mask (1 for real tokens, 0 for padding).\n",
    "6) Batch these tensors and feed to XLM-R encoder (Transformer stack).\n",
    "7) Take the hidden state at position 0 (the `<s>` token) as the sequence representation.\n",
    "8) Classification head (Dropout + Linear) → logits → Softmax → probabilities → predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09884b7b",
   "metadata": {},
   "source": [
    "## Tokenization and normalization\n",
    "- XLM-R uses a SentencePiece BPE tokenizer with a shared multilingual vocabulary.\n",
    "- It operates directly on raw text (no language-specific pre-tokenization needed).\n",
    "- Text is split into subword units; uncommon words become multiple pieces.\n",
    "- Special tokens used (RoBERTa family):\n",
    "  - `<s>`: start of sequence (equivalent to [CLS])\n",
    "  - `</s>`: end of sequence ([SEP])\n",
    "  - `<pad>`: padding token\n",
    "  - `<unk>`: unknown token\n",
    "  - `<mask>`: used only during masked language modeling pretraining\n",
    "- Unlike BERT, RoBERTa/XLM-R does not use token_type_ids (segment IDs), even for sentence pairs. Separation is done with `</s>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e24258d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['<s>', '▁This', '▁model', '▁support', 's', '▁many', '▁language', 's', '!', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
      "input_ids shape: torch.Size([1, 16])\n",
      "attention_mask shape: torch.Size([1, 16])\n"
     ]
    }
   ],
   "source": [
    "# Illustrative only: how raw text becomes tokens and ids\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')  # or your fine-tuned path\n",
    "text = \"This model supports many languages!\"\n",
    "enc = tokenizer(text, padding='max_length', truncation=True, max_length=16, return_tensors='pt')\n",
    "print('Tokens:', tokenizer.convert_ids_to_tokens(enc['input_ids'][0]))\n",
    "print('input_ids shape:', enc['input_ids'].shape)\n",
    "print('attention_mask shape:', enc['attention_mask'].shape)\n",
    "# Note: XLM-R does not use token_type_ids (it may be absent or all zeros)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654f37c8",
   "metadata": {},
   "source": [
    "## From text to tensors\n",
    "- input_ids: shape (batch_size, seq_len), integer IDs for each token.\n",
    "- attention_mask: same shape, 1 for tokens to attend to, 0 for paddings.\n",
    "- Padding/truncation: sequences are padded to the longest in the batch or a fixed max_length; longer sequences are truncated (commonly at 512 tokens).\n",
    "- For sentence pairs: format is `<s> sentence1 </s> </s> sentence2 </s>`; still no token_type_ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ca8dd1",
   "metadata": {},
   "source": [
    "## Batching and ingestion\n",
    "- Datasets are tokenized into dicts with keys like `input_ids`, `attention_mask`, `labels`.\n",
    "- A DataLoader (or Hugging Face Trainer) batches these tensors.\n",
    "- On each step, a batch (e.g., (B, L)) is moved to the target device (CPU/GPU) and passed to the model.\n",
    "- The attention_mask ensures the model does not attend to padding positions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742c61c",
   "metadata": {},
   "source": [
    "## Inside XLM-R (encoder) — what happens\n",
    "1) Embeddings layer:\n",
    "   - Token embeddings: lookup vectors for each input_id.\n",
    "   - Positional embeddings: add position information (0..L-1).\n",
    "   - (No segment/token_type embeddings).\n",
    "2) Transformer encoder stack (repeated N times, 12 in base):\n",
    "   - Multi-Head Self-Attention (MHSA) uses attention_mask to ignore pads.\n",
    "   - Add & LayerNorm (residual connection).\n",
    "   - Feed-Forward Network (GELU activation) per position.\n",
    "   - Add & LayerNorm again.\n",
    "3) Sequence representation:\n",
    "   - For classification, use the hidden state at position 0 (the `<s>` token) as a pooled representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b35dbe",
   "metadata": {},
   "source": [
    "## Classification head → logits → probabilities\n",
    "- Head: Dropout → Linear (hidden_size → num_labels).\n",
    "- Output: logits (shape (batch_size, num_labels)).\n",
    "- Training: CrossEntropyLoss compares logits vs. true `labels`. Class weights or focal loss can be used (as in your Phase 2 code).\n",
    "- Inference: apply Softmax to logits → probabilities per class.\n",
    "- Predicted label: argmax over probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c17c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probs: [[0.5814915299415588, 0.12974829971790314, 0.28876012563705444]]\n",
      "predicted class index: 0\n"
     ]
    }
   ],
   "source": [
    "# Illustrative only: logits to probabilities\n",
    "import torch, torch.nn.functional as F\n",
    "logits = torch.tensor([[1.2, -0.3, 0.5]])  # pretend output for 3 classes\n",
    "probs = F.softmax(logits, dim=-1)\n",
    "pred = torch.argmax(probs, dim=-1).item()\n",
    "print('probs:', probs.tolist())\n",
    "print('predicted class index:', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a04765",
   "metadata": {},
   "source": [
    "## Mapping to your Phase 2 notebook\n",
    "- Pre-processing: you clean text, drop nulls/duplicates, engineer labels.\n",
    "- Label encoding: you map label strings ↔ integers (`label_to_id`, `id_to_label`).\n",
    "- Tokenization: your `tokenize_dataset(...)` builds `input_ids` and `attention_mask` for each split.\n",
    "- Data format: `dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])` prepares tensors.\n",
    "- Trainer loop: batches tensors, feeds to model, computes loss (optionally weighted or focal), evaluates metrics.\n",
    "- Evaluation: you compute Accuracy, Macro-F1, and confusion matrices from predictions vs. labels.\n",
    "- Inference helpers: your `predict_text` / `predict_text_probs` wrap tokenizer → model → softmax → label mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eafcffa",
   "metadata": {},
   "source": [
    "## Practical notes and gotchas\n",
    "- Max length and truncation: long texts are truncated; consider summarizing or sliding windows when context is critical.\n",
    "- Tokenizer path: use your fine-tuned tokenizer/model paths for consistent vocabulary and special tokens.\n",
    "- Class imbalance: adjust loss (weights, focal loss) or rebalance data.\n",
    "- Multilingual inputs: XLM-R's shared vocabulary handles many languages, but domain-specific slang may be split into many subwords.\n",
    "- No token_type_ids: RoBERTa family ignores segment embeddings; rely on `</s>` separators for pairs.\n",
    "- Attention mask: always pass it to avoid attending to padding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2b5f2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Raw text → SentencePiece subwords + special tokens → IDs and masks → Transformer encoder → `<s>` representation → classification head → logits → Softmax probabilities.\n",
    "This is exactly how your Phase 2 pipeline turns text into final class probabilities and labels."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heron-wellnest-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
